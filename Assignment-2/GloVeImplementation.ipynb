{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GloVeImplementation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnEF7xJ6snI0",
        "outputId": "e4ea7813-a312-4d83-aeee-f653ab01bf8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget -c 'https://www.dropbox.com/s/1agrh5hdnkqd24c/en_wiki.txt?dl=0' -O en_wiki.txt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "! cp -R '/content/drive/My Drive/web' ./"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-05 17:17:45--  https://www.dropbox.com/s/1agrh5hdnkqd24c/en_wiki.txt?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.1, 2620:100:601f:1::a27d:901\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/1agrh5hdnkqd24c/en_wiki.txt [following]\n",
            "--2020-11-05 17:17:45--  https://www.dropbox.com/s/raw/1agrh5hdnkqd24c/en_wiki.txt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uca3d5dd66e4807ef2cf5b6145c5.dl.dropboxusercontent.com/cd/0/inline/BCovwoduYGjzx6DSqJTP-E1e8jBu2AX1M0rPB15J16x4L2KS-hbdPtoy86guu-BO3u6xna0mEtu7p8k-WKy_LVLS5ETqYWWRGafFRj3ESJf6OHLMm--GxIApE873nvWYijk/file# [following]\n",
            "--2020-11-05 17:17:45--  https://uca3d5dd66e4807ef2cf5b6145c5.dl.dropboxusercontent.com/cd/0/inline/BCovwoduYGjzx6DSqJTP-E1e8jBu2AX1M0rPB15J16x4L2KS-hbdPtoy86guu-BO3u6xna0mEtu7p8k-WKy_LVLS5ETqYWWRGafFRj3ESJf6OHLMm--GxIApE873nvWYijk/file\n",
            "Resolving uca3d5dd66e4807ef2cf5b6145c5.dl.dropboxusercontent.com (uca3d5dd66e4807ef2cf5b6145c5.dl.dropboxusercontent.com)... 162.125.9.15, 2620:100:601f:15::a27d:90f\n",
            "Connecting to uca3d5dd66e4807ef2cf5b6145c5.dl.dropboxusercontent.com (uca3d5dd66e4807ef2cf5b6145c5.dl.dropboxusercontent.com)|162.125.9.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeUsO2d3KTo_",
        "outputId": "2f5b8b7f-4b0e-4e0d-d86e-fddb2881d32e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "import random\n",
        "\n",
        "from web.datasets.similarity import fetch_MEN, fetch_WS353, fetch_SimLex999\n",
        "from web.embeddings import fetch_GloVe\n",
        "from web.evaluate import evaluate_similarity\n",
        "from six import iteritems"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.datasets.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0gJ9RIUsz5o"
      },
      "source": [
        "random.seed(4)\n",
        "text = open('/content/en_wiki.txt').read()\n",
        "text = text[:len(text)//5]\n",
        "text = text.replace('\\n','')\n",
        "sent_tokenized_corpus = sent_tokenize(text)\n",
        "tokenized_corpus = []\n",
        "for sent in sent_tokenized_corpus:\n",
        "  words = word_tokenize(sent)\n",
        "  tokenized_corpus.append(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbgLgEfS1toz"
      },
      "source": [
        "window_size = 4\n",
        "threshold = 10\n",
        "dimension = 100\n",
        "alpha = 0.75\n",
        "x_max = 100\n",
        "learning_rate = 0.01\n",
        "no_of_epochs = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn7_FJhoPEnv"
      },
      "source": [
        "# Steps:\n",
        "# build vocabulary with freq and index\n",
        "# build co-occurence matrix\n",
        "# train word vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytkp4HnFPoOB",
        "outputId": "25544f69-ac83-400b-803e-5b9db27c6c44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def get_vocabulary(data):\n",
        "  vocabulary_dict = defaultdict(int)\n",
        "  for sent in data:\n",
        "    for word in sent:\n",
        "      vocabulary_dict[word] += 1\n",
        "\n",
        "  vocab = {token:(index, frequency) for index, (token, frequency) in enumerate(vocabulary_dict.items())}\n",
        "  token_index_map = {token:index for token, (index,frequency) in vocab.items()}\n",
        "  return vocab, token_index_map\n",
        "\n",
        "testdata = [[\"My\", \"name\", \"is\", \"Umang.\"],[\"I\", \"am\", \"21\", \"years\", \"old.\"]]\n",
        "vocab, token_index_map = get_vocabulary(testdata)\n",
        "print(vocab)\n",
        "print(token_index_map)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'My': (0, 1), 'name': (1, 1), 'is': (2, 1), 'Umang.': (3, 1), 'I': (4, 1), 'am': (5, 1), '21': (6, 1), 'years': (7, 1), 'old.': (8, 1)}\n",
            "{'My': 0, 'name': 1, 'is': 2, 'Umang.': 3, 'I': 4, 'am': 5, '21': 6, 'years': 7, 'old.': 8}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unG47A89KIz5"
      },
      "source": [
        "def build_X(vocab, token_index_map, train_set, window_size, threshold):\n",
        "\n",
        "  vocab_size = len(vocab)\n",
        "\n",
        "  X = sparse.lil_matrix((vocab_size, vocab_size))\n",
        "  id2word = dict((i, word) for word, (i, _) in vocab.items())\n",
        "  for sent in train_set:\n",
        "    token_indices = [token_index_map[word] for word in sent]\n",
        "\n",
        "    for sent_index, token_index in enumerate(token_indices):\n",
        "\n",
        "      context_indices = token_indices[max(0, sent_index-window_size) : sent_index]\n",
        "      context_len = len(context_indices)\n",
        "\n",
        "      for left_sent_index, left_token_index in enumerate(context_indices):\n",
        "        dist = len(context_indices) - left_sent_index\n",
        "\n",
        "        X[token_index,left_token_index] += 1\n",
        "        X[left_token_index,token_index] += 1\n",
        "\n",
        "  X_tuples = []\n",
        "\n",
        "  for i, (row, data) in enumerate(zip(X.rows,X.data)):\n",
        "        if vocab[id2word[i]][1] < threshold:\n",
        "            continue\n",
        "\n",
        "        for index, j in enumerate(row):\n",
        "            if vocab[id2word[i]][1] < threshold:\n",
        "                continue\n",
        "\n",
        "            X_tuples.append((i, j, data[index])) \n",
        "\n",
        "  return X_tuples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJF5UqFzYB31"
      },
      "source": [
        "def epoch(vocab, data):\n",
        "\n",
        "  global_cost = 0\n",
        "  random.shuffle(data)\n",
        "\n",
        "  for (word_vector, context_vector, word_bias, context_bias, word_vector_gradsq, \n",
        "       context_vector_gradsq, word_bias_gradsq, context_bias_gradsq, x_ij) in data:\n",
        "\n",
        "    if x_ij < x_max:\n",
        "      weight = pow(x_ij/x_max, alpha)\n",
        "    else:\n",
        "      weight = 1\n",
        "\n",
        "      inner_cost = word_vector.dot(context_vector) + word_bias[0] + context_bias[0] - np.log(x_ij)\n",
        "\n",
        "      cost = weight*(pow(inner_cost,2))\n",
        "\n",
        "      global_cost += 0.5*cost\n",
        "      word_grad = inner_cost * context_vector\n",
        "      context_grad = inner_cost * word_vector\n",
        "      word_bias_grad = inner_cost\n",
        "      context_bias_grad = inner_cost\n",
        "\n",
        "      word_vector -= (learning_rate*word_grad)/np.sqrt(word_vector_gradsq)\n",
        "      context_vector -= (learning_rate*context_grad)/np.sqrt(context_vector_gradsq)\n",
        "      word_bias -= (learning_rate*word_bias_grad)/np.sqrt(word_bias_gradsq)\n",
        "      context_bias -= (learning_rate*context_bias_grad)/np.sqrt(context_bias_gradsq)\n",
        "\n",
        "      word_vector_gradsq += np.square(word_grad)\n",
        "      context_vector_gradsq += np.square(context_grad)\n",
        "      word_bias_gradsq += pow(word_bias_grad,2)\n",
        "      context_bias_gradsq += pow(context_bias_grad,2)\n",
        "\n",
        "  return global_cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZTFj9W8eMB5"
      },
      "source": [
        "def train_glove(vocab, X_tuples):\n",
        "\n",
        "  vocab_size = len(vocab)\n",
        "  W = (np.random.rand(vocab_size*2, dimension)-0.5)/float(dimension)\n",
        "  B = (np.random.rand(vocab_size*2)-0.5)/float(dimension)\n",
        "  gradient_sq_W = np.ones((vocab_size*2, dimension))\n",
        "  gradient_sq_B = np.ones((vocab_size*2))\n",
        "\n",
        "  data = []\n",
        "  for (word_index, context_index, x_ij) in X_tuples:\n",
        "    data_entry = ( W[word_index], W[context_index+vocab_size], B[word_index:word_index+1], B[context_index+vocab_size: context_index+vocab_size+1],\n",
        "                  gradient_sq_W[word_index], gradient_sq_W[context_index+vocab_size],\n",
        "                  gradient_sq_B[word_index: word_index+1], gradient_sq_B[context_index+vocab_size: context_index+vocab_size+1],\n",
        "                  x_ij)\n",
        "    data.append(data_entry)\n",
        "\n",
        "  for i in range(no_of_epochs):\n",
        "    print(\"Iteration Number:\",i+1)\n",
        "    cost = epoch(vocab, data)\n",
        "    print(\"Cost =\", cost)\n",
        "    print('')\n",
        "\n",
        "  return W,B"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsoosvHmnk2s"
      },
      "source": [
        "vocab, token_index_map = get_vocabulary(tokenized_corpus)\n",
        "# print(vocab['The'])\n",
        "# print(token_index_map['The'])\n",
        "X_tuples = build_X(vocab, token_index_map, tokenized_corpus, window_size, threshold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NytogsxFvmRK",
        "outputId": "e0af5cb4-a4e4-4505-b8fd-65a3f52a409b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "W, B = train_glove(vocab, X_tuples)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration Number: 1\n",
            "Cost = 241912.99212779765\n",
            "\n",
            "Iteration Number: 2\n",
            "Cost = 209048.29277624196\n",
            "\n",
            "Iteration Number: 3\n",
            "Cost = 136041.34292156497\n",
            "\n",
            "Iteration Number: 4\n",
            "Cost = 76095.61382862608\n",
            "\n",
            "Iteration Number: 5\n",
            "Cost = 53454.34053044682\n",
            "\n",
            "Iteration Number: 6\n",
            "Cost = 42731.68987090381\n",
            "\n",
            "Iteration Number: 7\n",
            "Cost = 36026.42721954118\n",
            "\n",
            "Iteration Number: 8\n",
            "Cost = 31214.91260206464\n",
            "\n",
            "Iteration Number: 9\n",
            "Cost = 27516.427733289416\n",
            "\n",
            "Iteration Number: 10\n",
            "Cost = 24557.443094084025\n",
            "\n",
            "Iteration Number: 11\n",
            "Cost = 22132.454892497106\n",
            "\n",
            "Iteration Number: 12\n",
            "Cost = 20108.22723551717\n",
            "\n",
            "Iteration Number: 13\n",
            "Cost = 18396.182518164038\n",
            "\n",
            "Iteration Number: 14\n",
            "Cost = 16931.85261266378\n",
            "\n",
            "Iteration Number: 15\n",
            "Cost = 15669.864382774802\n",
            "\n",
            "Iteration Number: 16\n",
            "Cost = 14573.9449782668\n",
            "\n",
            "Iteration Number: 17\n",
            "Cost = 13617.670373072317\n",
            "\n",
            "Iteration Number: 18\n",
            "Cost = 12778.120843805278\n",
            "\n",
            "Iteration Number: 19\n",
            "Cost = 12039.382841251712\n",
            "\n",
            "Iteration Number: 20\n",
            "Cost = 11385.886221033013\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIixvIpbC1fz"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "main_w = W[:vocab_size,:]\n",
        "context_w = W[vocab_size:, :]\n",
        "main_b = B[:vocab_size]\n",
        "context_b = B[vocab_size:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RU-HsLaIrRX",
        "outputId": "fb87599f-87a5-4013-8b6c-0d4d65e786f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "glove_vector = fetch_GloVe(corpus=\"wiki-6B\", dim=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset created in /root/web_data/embeddings\n",
            "\n",
            "Downloading data from http://nlp.stanford.edu/data/glove.6B.zip ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 862M/862M [06:27<00:00, 2.22Mb/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "...done. (388 seconds, 6 min)\n",
            "Extracting data from /root/web_data/embeddings/glove.6B/glove.6B.zip...\n",
            "   ...done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGoSVRM_awLt"
      },
      "source": [
        "tasks = {\n",
        "    \"MEN\": fetch_MEN(),\n",
        "    \"WS353\": fetch_WS353()\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uafk2WzKbXX_"
      },
      "source": [
        "vocab_vector = {}\n",
        "for token, id in token_index_map.items():\n",
        "  vocab_vector[token] = main_w[id]\n",
        "\n",
        "task_WS353 = [[],[]]\n",
        "name = \"WS353\"\n",
        "for i in range(len(tasks[name].X)):\n",
        "  if tasks[name].X[i][0] in vocab_vector and tasks[name].X[i][1] in vocab_vector:\n",
        "    task_WS353[0].append([tasks[name].X[i][0], tasks[name].X[i][1]])\n",
        "    task_WS353[1].append(tasks[name].y[i])\n",
        "task_WS353[0] = np.array(task_WS353[0])\n",
        "task_WS353[1] = np.array(task_WS353[1])\n",
        "\n",
        "\n",
        "task_MEN = [[], []]\n",
        "name = \"MEN\"\n",
        "\n",
        "for i in range(len(tasks[name].X)):\n",
        "  if tasks[name].X[i][0] in vocab_vector and tasks[name].X[i][1] in vocab_vector:\n",
        "    task_MEN[0].append([tasks[name].X[i][0], tasks[name].X[i][1]])\n",
        "    task_MEN[1].append(tasks[name].y[i][0])\n",
        "task_MEN[0] = np.array(task_MEN[0])\n",
        "task_MEN[1] = np.array(task_MEN[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcuRCtIHdmHa",
        "outputId": "a6027792-c195-43d5-a5c0-93c455245b63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "name = \"WS353\"\n",
        "print (\"Spearman correlation of scores on {}= {} for embeddings obtained using implementation\".format(name, evaluate_similarity(vocab_vector, task_WS353[0], task_WS353[1])))\n",
        "print (\"Spearman correlation of scores on {}= {} for pretrained embeddings\".format(name, evaluate_similarity(glove_vector, task_WS353[0], task_WS353[1])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/web/evaluate.py:336: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  A = np.vstack(w.get(word, mean_vector) for word in X[:, 0])\n",
            "/content/web/evaluate.py:337: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  B = np.vstack(w.get(word, mean_vector) for word in X[:, 1])\n",
            "Missing 21 words. Will replace them with mean vector\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Spearman correlation of scores on WS353= 0.022085059258130542 for embeddings obtained using implementation\n",
            "Spearman correlation of scores on WS353= 0.5316690635677356 for pretrained embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA_eq9_Hd30r",
        "outputId": "fd2d5c51-dd88-416e-bcf8-873d69aae043",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "name = \"MEN\"\n",
        "print (\"Spearman correlation of scores on {}= {} for embeddings obtained using implementation\".format(name, evaluate_similarity(vocab_vector, task_MEN[0], task_MEN[1])))\n",
        "print (\"Spearman correlation of scores on {}= {} for pretrained embeddings\".format(name, evaluate_similarity(glove_vector, task_MEN[0], task_MEN[1])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spearman correlation of scores on MEN= 0.02950118267341735 for embeddings obtained using implementation\n",
            "Spearman correlation of scores on MEN= 0.6965263061520983 for pretrained embeddings\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/web/evaluate.py:336: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  A = np.vstack(w.get(word, mean_vector) for word in X[:, 0])\n",
            "/content/web/evaluate.py:337: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  B = np.vstack(w.get(word, mean_vector) for word in X[:, 1])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTMS4wK9gzNC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}