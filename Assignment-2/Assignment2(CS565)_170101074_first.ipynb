{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2(CS565)_170101074_first.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmeD3pV4aN0o"
      },
      "source": [
        "# Assignment-2 Solutions for CS565\n",
        "This notebook contains the solutions for assignment on topics N-Gram Language Models, Smoothing and Vector Semantics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35ZzJ7BnZo2S",
        "outputId": "3f7f9105-c99d-48da-ddf6-3046ca90d0d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Downloading corpora\n",
        "!wget -c 'https://www.dropbox.com/s/1agrh5hdnkqd24c/en_wiki.txt?dl=0' -O en_wiki.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-05 14:49:07--  https://www.dropbox.com/s/1agrh5hdnkqd24c/en_wiki.txt?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.1, 2620:100:601d:1::a27d:501\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/1agrh5hdnkqd24c/en_wiki.txt [following]\n",
            "--2020-11-05 14:49:07--  https://www.dropbox.com/s/raw/1agrh5hdnkqd24c/en_wiki.txt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc12347d7ffc7302099da0212883.dl.dropboxusercontent.com/cd/0/inline/BCoc0kKRN2WLRiiFCGY5s0vvwNwV9jjElGnIL4gg_eGwTQQZEJ9HzchQh4wnGvQCjJ4dI_5NBowovAJjEEk03ix6UMipsiVxdC24Gd88QUOdHe5Qkz4fUDo_qKMEO1_yjgs/file# [following]\n",
            "--2020-11-05 14:49:08--  https://uc12347d7ffc7302099da0212883.dl.dropboxusercontent.com/cd/0/inline/BCoc0kKRN2WLRiiFCGY5s0vvwNwV9jjElGnIL4gg_eGwTQQZEJ9HzchQh4wnGvQCjJ4dI_5NBowovAJjEEk03ix6UMipsiVxdC24Gd88QUOdHe5Qkz4fUDo_qKMEO1_yjgs/file\n",
            "Resolving uc12347d7ffc7302099da0212883.dl.dropboxusercontent.com (uc12347d7ffc7302099da0212883.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uc12347d7ffc7302099da0212883.dl.dropboxusercontent.com (uc12347d7ffc7302099da0212883.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 103092525 (98M) [text/plain]\n",
            "Saving to: ‘en_wiki.txt’\n",
            "\n",
            "en_wiki.txt         100%[===================>]  98.32M  60.0MB/s    in 1.6s    \n",
            "\n",
            "2020-11-05 14:49:10 (60.0 MB/s) - ‘en_wiki.txt’ saved [103092525/103092525]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5mVQ54VaoO7",
        "outputId": "dc825721-895f-4c84-e42c-556e781ea05c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# imprting all required libraries\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk import *\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import random\n",
        "from math import floor\n",
        "import numpy as np\n",
        "import copy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqVV4zIkcup-"
      },
      "source": [
        "## Dividing training and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEkWSO_ab8gI"
      },
      "source": [
        "# Reading text from given corpus\n",
        "\n",
        "text = open('/content/en_wiki.txt').read()\n",
        "\n",
        "# tokenizing corpora and shuffling the list\n",
        "sent_tokenize_list = sent_tokenize(text)\n",
        "sent_tokenize_list = sent_tokenize_list[0:1500]\n",
        "random.seed(4)\n",
        "random.shuffle(sent_tokenize_list)\n",
        "\n",
        "data = []\n",
        "for sent in sent_tokenize_list:\n",
        "  tokenized_sentence = [\"START\", \"START\"]\n",
        "  words = word_tokenize(sent)\n",
        "  for word in words:\n",
        "    if word.isalnum():\n",
        "      tokenized_sentence.append(word)\n",
        "  if len(tokenized_sentence) > 2:\n",
        "    tokenized_sentence.append(\"STOP\")\n",
        "    data.append(tokenized_sentence)\n",
        "\n",
        "# print(data)\n",
        "# dividing corpora into training and test sets\n",
        "\n",
        "division_index = floor(0.9*len(data))\n",
        "train_dev_set = data[0:division_index]\n",
        "test_set = data[division_index:]\n",
        "\n",
        "# print(len(data))\n",
        "# print(data[:10])\n",
        "# print(len(train_dev_set))\n",
        "# print(train_dev_set[:10])\n",
        "# print(len(test_set))\n",
        "# print(test_set[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7kKhCfurOBt"
      },
      "source": [
        "## Trigram language model using interpolation smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L27mw2eHyxjM"
      },
      "source": [
        "# Steps:\n",
        "# 1. generate vocabulary\n",
        "# 2. unknown word mapping\n",
        "# 3. get lambda from training set\n",
        "# 4. calculate perplexity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IM20Fvu0Zqa"
      },
      "source": [
        "def get_vocabulary(data, freq_threshold):\n",
        "\n",
        "  vocabulary_dict = defaultdict(int)\n",
        "  for sent in data:\n",
        "    for word in sent:\n",
        "      vocabulary_dict[word] += 1\n",
        "\n",
        "  temp_vocab = defaultdict(int)\n",
        "  for key, value in vocabulary_dict.items():\n",
        "    if value <= freq_threshold:\n",
        "      temp_vocab[\"UNKNOWN\"] += 1\n",
        "    else:\n",
        "      temp_vocab[key] = value\n",
        "\n",
        "  return list(temp_vocab.keys())\n",
        "\n",
        "vocab = get_vocabulary(train_dev_set, 6)\n",
        "# print(vocab)\n",
        "# print(len(vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8MWZ36O3XeD"
      },
      "source": [
        "def map_unknown_words(data, vocabulary):\n",
        "  no_of_sentences = len(data)\n",
        "  for row in range(no_of_sentences):\n",
        "    no_of_words = len(data[row])\n",
        "    for col in range(no_of_words):\n",
        "      if data[row][col] not in vocabulary:\n",
        "        data[row][col] = \"UNKNOWN\"\n",
        "\n",
        "# tdata = [[\"START\", \"START\", \"my\", \"name\", \"is\", \"Umang\", \"STOP\"], [\"START\", \"START\", \"name\", \"am\", \"21\", \"years\", \"is\", \"old\", \"STOP\"]]\n",
        "# map_unknown_words(tdata, vocab)\n",
        "# print(tdata)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1w5tsKzf4KK"
      },
      "source": [
        "def calculate_perplexity(lambda1, lambda2, lambda3, test_set, count_trigram_train, count_bigram_train, count_unigram_train, no_of_words_train):\n",
        "  \n",
        "  trigram_list_test = []\n",
        "  for sent in test_set:\n",
        "    trigram_list_test.extend(list(nltk.trigrams(sent)))\n",
        "\n",
        "  count_trigram_test = defaultdict(int,Counter(trigram_list_test))\n",
        "\n",
        "  test_trigram_count_array = []\n",
        "  trigram_prob_array = []\n",
        "  bigram_prob_array = []\n",
        "  unigram_prob_array = []\n",
        "\n",
        "  trigram_list = list(count_trigram_test.keys())\n",
        "  for trigram in trigram_list:\n",
        "\n",
        "    count = count_trigram_test[trigram]\n",
        "    u = trigram[0]\n",
        "    v = trigram[1]\n",
        "    w = trigram[2]\n",
        "\n",
        "    test_trigram_count_array.append(count)\n",
        "\n",
        "    if count_bigram_train[(u,v)] != 0:\n",
        "      trigram_prob_array.append(count_trigram_train[trigram]/count_bigram_train[(u,v)])\n",
        "    else:\n",
        "      trigram_prob_array.append(0)\n",
        "\n",
        "    if count_unigram_train[v] != 0:\n",
        "      bigram_prob_array.append(count_bigram_train[(v, w)]/count_unigram_train[v])\n",
        "    else:\n",
        "      bigram_prob_array.append(0)\n",
        "\n",
        "    unigram_prob_array.append(count_unigram_train[w]/no_of_words_train)\n",
        "\n",
        "    prob = lambda1*np.array(trigram_prob_array) + lambda2*np.array(bigram_prob_array) + lambda3*np.array(unigram_prob_array)\n",
        "    log_term = np.ma.log2(prob)\n",
        "    log_term = log_term.filled(0)\n",
        "\n",
        "    no_of_words_test = sum(len(sent)-3 for sent in test_set)\n",
        "    L = np.dot(np.array(test_trigram_count_array), log_term)/no_of_words_test\n",
        "\n",
        "    return pow(2,-1*L), L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0Z32dLJA65t"
      },
      "source": [
        "def get_trained_lambdas(train_set, dev_set, test_set):\n",
        "\n",
        "  trigram_list_train = []\n",
        "  bigram_list_train = []\n",
        "  unigram_list_train = []\n",
        "\n",
        "  for sent in train_set:\n",
        "    unigram_list_train.extend(sent)\n",
        "    bigram_list_train.extend(list(nltk.bigrams(sent)))\n",
        "    trigram_list_train.extend(list(nltk.trigrams(sent)))\n",
        "\n",
        "  count_trigram_train = defaultdict(int,Counter(trigram_list_train))\n",
        "  count_bigram_train = defaultdict(int,Counter(bigram_list_train))\n",
        "  count_unigram_train = defaultdict(int,Counter(unigram_list_train))\n",
        "\n",
        "  trigram_list_dev = []\n",
        "  for sent in dev_set:\n",
        "    trigram_list_dev.extend(list(nltk.trigrams(sent)))\n",
        "\n",
        "  count_trigram_dev = defaultdict(int,Counter(trigram_list_dev))\n",
        "\n",
        "  dev_trigram_count_array = []\n",
        "  trigram_prob_array = []\n",
        "  bigram_prob_array = []\n",
        "  unigram_prob_array = []\n",
        "\n",
        "  no_of_words_train = sum(len(sent)-3 for sent in train_set)\n",
        "\n",
        "  trigram_list = list(count_trigram_dev.keys())\n",
        "  for trigram in trigram_list:\n",
        "\n",
        "    count = count_trigram_dev[trigram]\n",
        "    u = trigram[0]\n",
        "    v = trigram[1]\n",
        "    w = trigram[2]\n",
        "\n",
        "    dev_trigram_count_array.append(count)\n",
        "\n",
        "    if count_bigram_train[(u,v)] != 0:\n",
        "      trigram_prob_array.append(count_trigram_train[trigram]/count_bigram_train[(u,v)])\n",
        "    else:\n",
        "      trigram_prob_array.append(0)\n",
        "\n",
        "    if count_unigram_train[v] != 0:\n",
        "      bigram_prob_array.append(count_bigram_train[(v, w)]/count_unigram_train[v])\n",
        "    else:\n",
        "      bigram_prob_array.append(0)\n",
        "\n",
        "    unigram_prob_array.append(count_unigram_train[w]/no_of_words_train)\n",
        "\n",
        "  maxL = float('-inf')\n",
        "  lambda1 = -1\n",
        "  lambda2 = -1\n",
        "  lambda3 = -1\n",
        "\n",
        "  step = 0.1\n",
        "  for l1 in np.arange(0.1,1,step):\n",
        "    for l2 in np.arange(0,1-l1,step):\n",
        "      l3 = 1-l1-l2\n",
        "\n",
        "      prob = l1*np.array(trigram_prob_array) + l2*np.array(bigram_prob_array) + l3*np.array(unigram_prob_array)\n",
        "      log_term = np.ma.log2(prob)\n",
        "      log_term = log_term.filled(0)\n",
        "\n",
        "      L = np.dot(np.array(dev_trigram_count_array), log_term)\n",
        "\n",
        "      if L > maxL:\n",
        "        # print('Updating maxL to ',L)\n",
        "        maxL = L\n",
        "        lambda1 = l1\n",
        "        lambda2 = l2\n",
        "        lambda3 = l3\n",
        "\n",
        "  perplexity_dev, log_likelihood_dev = calculate_perplexity(lambda1, lambda2, lambda3, dev_set, count_trigram_train, count_bigram_train, count_unigram_train, no_of_words_train)\n",
        "  perplexity_test, log_likelihood_test = calculate_perplexity(lambda1, lambda2, lambda3, test_set, count_trigram_train, count_bigram_train, count_unigram_train, no_of_words_train)\n",
        "  return lambda1, lambda2, lambda3, perplexity_dev, perplexity_test, log_likelihood_dev, log_likelihood_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzGb5fVWfh8g"
      },
      "source": [
        "map_unknown_words(train_dev_set, vocab)\n",
        "map_unknown_words(test_set, vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENwabbz0g15C"
      },
      "source": [
        "# print(len(train_dev_set))\n",
        "# print(train_dev_set[:100])\n",
        "# print(len(test_set))\n",
        "# print(test_set[:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eSadW2zX42W",
        "outputId": "d30cb99f-694c-4ca6-a3e8-0247b41b328e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in range(5):\n",
        "\n",
        "  data = copy.deepcopy(train_dev_set)\n",
        "  random.shuffle(data)\n",
        "\n",
        "  division_index = floor(0.9*len(data))\n",
        "  train_set = data[:division_index]\n",
        "  dev_set = data[division_index:]\n",
        "  test_set_copy = copy.deepcopy(test_set)\n",
        "\n",
        "  lambda1, lambda2, lambda3, perplexity_dev, perplexity_test, log_likelihood_dev, log_likelihood_test = get_trained_lambdas(train_set, dev_set, test_set_copy)\n",
        "\n",
        "  print('Iteration Number: ', i+1)\n",
        "  print('(lambda1, lambda2, lambda3) = (',round(lambda1,2),',',round(lambda2,2),',',round(lambda3,2),')')\n",
        "  print('Log-likelihood for validation set =', log_likelihood_dev)\n",
        "  print('Perplexity for validation set =', perplexity_dev)\n",
        "  print('Log-likelihood for test set =', log_likelihood_test)\n",
        "  print('Perplexity for test set =', perplexity_test)\n",
        "  print('')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration Number:  1\n",
            "(lambda1, lambda2, lambda3) = ( 0.1 , 0.5 , 0.4 )\n",
            "Log-likelihood for validation set = -0.005868830617309913\n",
            "Perplexity for validation set = 1.0040762487897499\n",
            "Log-likelihood for test set = -0.03490225060198852\n",
            "Perplexity for test set = 1.0244874068241732\n",
            "\n",
            "Iteration Number:  2\n",
            "(lambda1, lambda2, lambda3) = ( 0.1 , 0.5 , 0.4 )\n",
            "Log-likelihood for validation set = -0.030006145581610702\n",
            "Perplexity for validation set = 1.02101647501632\n",
            "Log-likelihood for test set = -0.03503212004305289\n",
            "Perplexity for test set = 1.0245796339350888\n",
            "\n",
            "Iteration Number:  3\n",
            "(lambda1, lambda2, lambda3) = ( 0.1 , 0.5 , 0.4 )\n",
            "Log-likelihood for validation set = -0.028970345760187248\n",
            "Perplexity for validation set = 1.0202836873540269\n",
            "Log-likelihood for test set = -0.0354895794367385\n",
            "Perplexity for test set = 1.0249045660118785\n",
            "\n",
            "Iteration Number:  4\n",
            "(lambda1, lambda2, lambda3) = ( 0.1 , 0.5 , 0.4 )\n",
            "Log-likelihood for validation set = -0.023277940534675953\n",
            "Perplexity for validation set = 1.0162659115213104\n",
            "Log-likelihood for test set = -0.034969763468450765\n",
            "Perplexity for test set = 1.024535350220327\n",
            "\n",
            "Iteration Number:  5\n",
            "(lambda1, lambda2, lambda3) = ( 0.1 , 0.5 , 0.4 )\n",
            "Log-likelihood for validation set = -0.015700840288278603\n",
            "Perplexity for validation set = 1.0109424283637245\n",
            "Log-likelihood for test set = -0.0354180015763162\n",
            "Perplexity for test set = 1.024853717666199\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xviPh89G3k1V"
      },
      "source": [
        "## Trigram language model using discounting smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeIeCxYToJii"
      },
      "source": [
        "# Steps:\n",
        "# 1. calculate unigram probabilities\n",
        "# for each beta\n",
        "#   2. estimate bigram discounted prob\n",
        "#   3. trigram discount prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To78nA84reih"
      },
      "source": [
        "# Reading text from given corpus\n",
        "\n",
        "text = open('/content/en_wiki.txt').read()\n",
        "text = text[:len(text)//700]\n",
        "# tokenizing corpora and shuffling the list\n",
        "sent_tokenize_list = sent_tokenize(text)\n",
        "random.seed(4)\n",
        "random.shuffle(sent_tokenize_list)\n",
        "# sent_tokenize_list = [\"My name is Umang.\", \"I am 21 years old.\"]\n",
        "\n",
        "data = []\n",
        "for sent in sent_tokenize_list:\n",
        "  tokenized_sentence = [\"START\", \"START\"]\n",
        "  words = word_tokenize(sent)\n",
        "  for word in words:\n",
        "    if word.isalnum():\n",
        "      tokenized_sentence.append(word)\n",
        "  if len(tokenized_sentence) > 2:\n",
        "    tokenized_sentence.append(\"STOP\")\n",
        "    data.append(tokenized_sentence)\n",
        "\n",
        "# print(data)\n",
        "# dividing corpora into training and test sets\n",
        "\n",
        "division_index = floor(0.9*len(data))\n",
        "train_dev_set = data[0:division_index]\n",
        "test_set = data[division_index:]\n",
        "\n",
        "# print(len(data))\n",
        "# print(data[:10])\n",
        "# print(len(train_dev_set))\n",
        "# print(train_dev_set[:10])\n",
        "# print(len(test_set))\n",
        "# print(test_set[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd-WsDmVrva8"
      },
      "source": [
        "def get_vocabulary(data, freq_threshold):\n",
        "\n",
        "  vocabulary_dict = defaultdict(int)\n",
        "  for sent in data:\n",
        "    for word in sent:\n",
        "      vocabulary_dict[word] += 1\n",
        "\n",
        "  temp_vocab = defaultdict(int)\n",
        "  for key, value in vocabulary_dict.items():\n",
        "    if value <= freq_threshold:\n",
        "      temp_vocab[\"UNKNOWN\"] += 1\n",
        "    else:\n",
        "      temp_vocab[key] = value\n",
        "\n",
        "  return list(temp_vocab.keys())\n",
        "\n",
        "vocab = get_vocabulary(train_dev_set, 9)\n",
        "# print(vocab)\n",
        "# print(len(vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBYOCOpesPXE"
      },
      "source": [
        "def map_unknown_words(data, vocabulary):\n",
        "  no_of_sentences = len(data)\n",
        "  for row in range(no_of_sentences):\n",
        "    no_of_words = len(data[row])\n",
        "    for col in range(no_of_words):\n",
        "      if data[row][col] not in vocabulary:\n",
        "        data[row][col] = \"UNKNOWN\"\n",
        "\n",
        "# tdata = [[\"START\", \"START\", \"my\", \"name\", \"is\", \"A\", \"STOP\"], [\"START\", \"START\", \"name\", \"am\", \"years\", \"is\", \"old\", \"STOP\"]]\n",
        "# map_unknown_words(tdata, vocab)\n",
        "# print(tdata)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUi-YhzLsRU0",
        "cellView": "code"
      },
      "source": [
        "map_unknown_words(train_dev_set, vocab)\n",
        "map_unknown_words(test_set, vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl5K87TRsbug"
      },
      "source": [
        "def get_unigram_prob(count_unigram_train, no_of_words_train):\n",
        "  prob_dict = defaultdict(int)\n",
        "  unigrams = list(count_unigram_train.keys())\n",
        "  for unigram in unigrams:\n",
        "    prob_dict[unigram] = count_unigram_train[unigram]/no_of_words_train\n",
        "  return prob_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bKngxaqxHwk"
      },
      "source": [
        "def get_bigram_discounted_prob_beta(count_unigram_train, count_bigram_train, no_of_words_train, beta, unigram_prob):\n",
        "  unigrams = list(count_unigram_train.keys())\n",
        "\n",
        "  prob_dict = {}\n",
        "  for v in unigrams:\n",
        "    B = {}\n",
        "    sigma = 0\n",
        "    prob_sum = 0\n",
        "    for w in unigrams:\n",
        "      if count_bigram_train[(v,w)] > 0:\n",
        "        prob_dict[(v,w)] = (count_bigram_train[(v,w)] - beta)/count_unigram_train[v]\n",
        "        prob_sum += prob_dict[(v,w)]\n",
        "      elif count_bigram_train[(v,w)] == 0:\n",
        "        # B[(v,w)] = unigram_prob[w]\n",
        "        sigma += unigram_prob[w]\n",
        "\n",
        "    # sigma = sum(list(B.values()))\n",
        "    # alpha = 1-sum(list(prob_dict.values()))\n",
        "    alpha = 1 - prob_sum\n",
        "    # print(sigma,alpha)\n",
        "    for w in unigrams:\n",
        "      if count_bigram_train[(v,w)] == 0:\n",
        "        prob_dict[(v,w)] = alpha*(unigram_prob[w]/sigma)\n",
        "  return prob_dict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tllz_SLt9Z83"
      },
      "source": [
        "def get_trigram_discounted_prob_beta(count_bigram_train, count_trigram_train, no_of_words_train, beta, bigram_prob, count_unigram_train):\n",
        "  unigrams = list(count_unigram_train.keys())\n",
        "\n",
        "  prob_dict = {}\n",
        "  for u in unigrams:\n",
        "    for v in unigrams:\n",
        "      sigma = 0\n",
        "      prob_sum = 0\n",
        "\n",
        "      for w in unigrams:\n",
        "        if count_trigram_train[(u,v,w)] > 0:\n",
        "          prob_dict[(u,v,w)] = (count_trigram_train[(u,v,w)]-beta)/count_bigram_train[(u,v)]\n",
        "          prob_sum += prob_dict[(u,v,w)]\n",
        "        elif count_trigram_train[(u,v,w)] == 0:\n",
        "          sigma += bigram_prob[(v,w)]\n",
        "\n",
        "      alpha = 1-prob_sum\n",
        "\n",
        "      for w in unigrams:\n",
        "        if count_trigram_train[(u,v,w)] == 0:\n",
        "          prob_dict[(u,v,w)] = alpha*(bigram_prob[(v,w)]/sigma)\n",
        "  \n",
        "  return prob_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxRMBcOX8-CB"
      },
      "source": [
        "def get_bigram_discounted_prob(count_unigram_train, count_bigram_train, no_of_words_train, unigram_prob, count_bigram_dev):\n",
        "\n",
        "  best_beta = 0\n",
        "  maxL = float('-inf')\n",
        "  bigram_discounted_prob = {}\n",
        "  step = 0.1\n",
        "  for beta in np.arange(0.1,1,step):\n",
        "    L = 0\n",
        "    cur_bigram_prob = get_bigram_discounted_prob_beta(count_unigram_train, count_bigram_train, no_of_words_train, beta, unigram_prob)\n",
        "    bigram_list = list(count_bigram_dev.keys())\n",
        "    for bigram in bigram_list:\n",
        "      count = count_bigram_dev[bigram]\n",
        "      if cur_bigram_prob[bigram] !=0 :\n",
        "        L += count*np.log2(cur_bigram_prob[bigram])\n",
        "    if L > maxL:\n",
        "      maxL = L\n",
        "      best_beta = beta\n",
        "      bigram_discounted_prob = cur_bigram_prob\n",
        "\n",
        "  return beta, bigram_discounted_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUG56FaH7bdM"
      },
      "source": [
        "def get_trigram_discounted_prob(count_bigram_train, count_trigram_train, no_of_words_train, bigram_prob, count_trigram_dev, count_unigram_train, no_of_words_dev):\n",
        "\n",
        "  best_beta = 0\n",
        "  maxL = float('-inf')\n",
        "  trigram_discounted_prob = {}\n",
        "  step = 0.1\n",
        "  for beta in np.arange(0.1,1,step):\n",
        "    L = 0\n",
        "    cur_trigram_prob = get_trigram_discounted_prob_beta(count_bigram_train, count_trigram_train, no_of_words_train, beta, bigram_prob, count_unigram_train)\n",
        "    trigram_list = list(count_trigram_dev.keys())\n",
        "    for trigram in trigram_list:\n",
        "      count = count_trigram_dev[trigram]\n",
        "      if cur_trigram_prob[trigram] !=0 :\n",
        "        L += count*np.log2(cur_trigram_prob[trigram])\n",
        "    if L > maxL:\n",
        "      maxL = L\n",
        "      best_beta = beta\n",
        "      trigram_discounted_prob = cur_trigram_prob\n",
        "\n",
        "  return beta, trigram_discounted_prob, maxL/no_of_words_dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy2oZEgMak8E"
      },
      "source": [
        "def calculate_perplexity_discounting(count_trigram_test, trigram_prob, no_of_words_test):\n",
        "\n",
        "  trigrams = list(count_trigram_test.keys())\n",
        "  L = 0\n",
        "  for trigram in trigrams:\n",
        "    count = count_trigram_test[trigram]\n",
        "    if trigram_prob[trigram] !=0 :\n",
        "      L += count*np.log2(trigram_prob[trigram])\n",
        "\n",
        "  L = L/no_of_words_test\n",
        "  return pow(2,-1*L), L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAwgvdUqshTl",
        "outputId": "03dc7e3e-f35b-4137-f9c2-c19bba49dbbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in range(5):\n",
        "\n",
        "  data = copy.deepcopy(train_dev_set)\n",
        "  random.shuffle(data)\n",
        "\n",
        "  division_index = floor(0.9*len(data))\n",
        "  train_set = data[:division_index]\n",
        "  dev_set = data[division_index:]\n",
        "  test_set_copy = copy.deepcopy(test_set)\n",
        "\n",
        "  trigram_list_train = []\n",
        "  bigram_list_train = []\n",
        "  unigram_list_train = []\n",
        "\n",
        "  for sent in train_set:\n",
        "    unigram_list_train.extend(sent)\n",
        "    bigram_list_train.extend(list(nltk.bigrams(sent)))\n",
        "    trigram_list_train.extend(list(nltk.trigrams(sent)))\n",
        "\n",
        "  count_trigram_train = defaultdict(int,Counter(trigram_list_train))\n",
        "  count_bigram_train = defaultdict(int,Counter(bigram_list_train))\n",
        "  count_unigram_train = defaultdict(int,Counter(unigram_list_train))\n",
        "\n",
        "  trigram_list_dev = []\n",
        "  bigram_list_dev = []\n",
        "\n",
        "  for sent in dev_set:\n",
        "    bigram_list_dev.extend(list(nltk.bigrams(sent)))\n",
        "    trigram_list_dev.extend(list(nltk.trigrams(sent)))\n",
        "\n",
        "  count_trigram_dev = defaultdict(int,Counter(trigram_list_dev))\n",
        "  count_bigram_dev = defaultdict(int,Counter(bigram_list_dev))\n",
        "\n",
        "  trigram_list_test = []\n",
        "\n",
        "  for sent in test_set_copy:\n",
        "    trigram_list_test.extend(list(nltk.trigrams(sent)))\n",
        "  \n",
        "  count_trigram_test = defaultdict(int,Counter(trigram_list_test))\n",
        "\n",
        "  no_of_words_train = 0\n",
        "  for sent in train_set:\n",
        "    no_of_words_train += len(sent)\n",
        "  \n",
        "  no_of_words_dev = 0\n",
        "  for sent in dev_set:\n",
        "    no_of_words_dev += len(sent)\n",
        "\n",
        "  no_of_words_test = 0\n",
        "  for sent in test_set:\n",
        "    no_of_words_test += len(sent)\n",
        "\n",
        "  unigram_prob = get_unigram_prob(count_unigram_train, no_of_words_train)\n",
        "  beta_bigram, bigram_prob = get_bigram_discounted_prob(count_unigram_train, count_bigram_train, no_of_words_train, unigram_prob, count_bigram_dev)\n",
        "  # print(sum(list(bigram_prob.values())))\n",
        "  beta_trigram, trigram_prob, log_likelihood_dev = get_trigram_discounted_prob(count_bigram_train, count_trigram_train, no_of_words_train, bigram_prob, count_trigram_dev, count_unigram_train, no_of_words_dev)\n",
        "  # print(sum(list(trigram_prob.values())))\n",
        "  perplexity_test, log_likelihood_test = calculate_perplexity_discounting(count_trigram_test, trigram_prob, no_of_words_test)\n",
        "  print('Iteration Number: ', i+1)\n",
        "  print('(beta(bigram), beta(trigram)) = (',round(beta_bigram,2),',',round(beta_trigram,2),')')\n",
        "  print('Log-likelihood for validation set =', log_likelihood_dev)\n",
        "  print('Perplexity for validation set =', pow(2,-1*log_likelihood_dev))\n",
        "  print('Log-likelihood for test set =', log_likelihood_test)\n",
        "  print('Perplexity for test set =', perplexity_test)\n",
        "  print('')\n",
        "\n",
        "  del data, train_set, dev_set, test_set_copy, trigram_list_train, bigram_list_train, unigram_list_train, count_trigram_train, count_bigram_train, count_unigram_train\n",
        "  del trigram_list_dev, bigram_list_dev, count_trigram_dev, count_bigram_dev, trigram_list_test, count_trigram_test, no_of_words_train, no_of_words_dev, no_of_words_test\n",
        "  del unigram_prob, beta_bigram, bigram_prob, beta_trigram, trigram_prob, log_likelihood_dev, perplexity_test, log_likelihood_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration Number:  1\n",
            "(beta(bigram), beta(trigram)) = ( 0.9 , 0.9 )\n",
            "Log-likelihood for validation set = -3.92736746456683\n",
            "Perplexity for validation set = 15.214420345992108\n",
            "Log-likelihood for test set = -3.748243127829662\n",
            "Perplexity for test set = 13.437968307997425\n",
            "\n",
            "Iteration Number:  2\n",
            "(beta(bigram), beta(trigram)) = ( 0.9 , 0.9 )\n",
            "Log-likelihood for validation set = -3.8622204405850886\n",
            "Perplexity for validation set = 14.542671781238463\n",
            "Log-likelihood for test set = -3.75942235417423\n",
            "Perplexity for test set = 13.542501581493012\n",
            "\n",
            "Iteration Number:  3\n",
            "(beta(bigram), beta(trigram)) = ( 0.9 , 0.9 )\n",
            "Log-likelihood for validation set = -3.8761368011263206\n",
            "Perplexity for validation set = 14.683630401481283\n",
            "Log-likelihood for test set = -3.7526452469163925\n",
            "Perplexity for test set = 13.479034422520657\n",
            "\n",
            "Iteration Number:  4\n",
            "(beta(bigram), beta(trigram)) = ( 0.9 , 0.9 )\n",
            "Log-likelihood for validation set = -3.9186065207751137\n",
            "Perplexity for validation set = 15.122308861883775\n",
            "Log-likelihood for test set = -3.76589079537882\n",
            "Perplexity for test set = 13.603356817656842\n",
            "\n",
            "Iteration Number:  5\n",
            "(beta(bigram), beta(trigram)) = ( 0.9 , 0.9 )\n",
            "Log-likelihood for validation set = -3.8363373685974556\n",
            "Perplexity for validation set = 14.284091432003562\n",
            "Log-likelihood for test set = -3.7717731003443715\n",
            "Perplexity for test set = 13.65893505427004\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9NYnlUZ391l"
      },
      "source": [
        "## Trigram language model using laplace smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8EIK0Golxi1"
      },
      "source": [
        "# Steps:\n",
        "# 1. calculate counts on train set\n",
        "# 2. calculate probabilities\n",
        "# 3. calculate likelihood"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wsbJjoz4qgO"
      },
      "source": [
        "# Reading text from given corpus\n",
        "\n",
        "text = open('/content/en_wiki.txt').read()\n",
        "\n",
        "# tokenizing corpora and shuffling the list\n",
        "sent_tokenize_list = sent_tokenize(text)\n",
        "sent_tokenize_list = sent_tokenize_list[0:1200]\n",
        "random.seed(4)\n",
        "random.shuffle(sent_tokenize_list)\n",
        "# sent_tokenize_list = [\"My name is Umang.\", \"I am 21 years old.\"]\n",
        "\n",
        "data = []\n",
        "for sent in sent_tokenize_list:\n",
        "  tokenized_sentence = [\"START\", \"START\"]\n",
        "  words = word_tokenize(sent)\n",
        "  for word in words:\n",
        "    if word.isalnum():\n",
        "      tokenized_sentence.append(word)\n",
        "  if len(tokenized_sentence) > 2:\n",
        "    tokenized_sentence.append(\"STOP\")\n",
        "    data.append(tokenized_sentence)\n",
        "\n",
        "# print(data)\n",
        "# dividing corpora into training and test sets\n",
        "\n",
        "division_index = floor(0.9*len(data))\n",
        "train_dev_set = data[0:division_index]\n",
        "test_set = data[division_index:]\n",
        "\n",
        "# print(len(data))\n",
        "# print(data[:10])\n",
        "# print(len(train_dev_set))\n",
        "# print(train_dev_set[:10])\n",
        "# print(len(test_set))\n",
        "# print(test_set[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3o6XVta7ow7"
      },
      "source": [
        "def get_vocabulary(data, freq_threshold):\n",
        "\n",
        "  vocabulary_dict = defaultdict(int)\n",
        "  for sent in data:\n",
        "    for word in sent:\n",
        "      vocabulary_dict[word] += 1\n",
        "\n",
        "  temp_vocab = defaultdict(int)\n",
        "  for key, value in vocabulary_dict.items():\n",
        "    if value <= freq_threshold:\n",
        "      temp_vocab[\"UNKNOWN\"] += 1\n",
        "    else:\n",
        "      temp_vocab[key] = value\n",
        "\n",
        "  return list(temp_vocab.keys())\n",
        "\n",
        "vocab = get_vocabulary(train_dev_set, 9)\n",
        "# print(vocab)\n",
        "# print(len(vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKKulyLS7plK"
      },
      "source": [
        "def map_unknown_words(data, vocabulary):\n",
        "  no_of_sentences = len(data)\n",
        "  for row in range(no_of_sentences):\n",
        "    no_of_words = len(data[row])\n",
        "    for col in range(no_of_words):\n",
        "      if data[row][col] not in vocabulary:\n",
        "        data[row][col] = \"UNKNOWN\"\n",
        "\n",
        "map_unknown_words(train_dev_set, vocab)\n",
        "map_unknown_words(test_set, vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDqdcf7Q9-xq"
      },
      "source": [
        "def get_trigram_probabilities(count_unigram_train, count_bigram_train, count_trigram_train, V):\n",
        "  prob_dict = defaultdict(int)\n",
        "  unigrams = list(count_unigram_train.keys())\n",
        "\n",
        "  for u in unigrams:\n",
        "    for v in unigrams:\n",
        "      for w in unigrams:\n",
        "        prob_dict[(u,v,w)] = (count_trigram_train[(u,v,w)] + 1)/(count_bigram_train[(u,v)] + V)\n",
        "  return prob_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpqPSp58_uyz"
      },
      "source": [
        "def compute_perplexity(count_trigram, prob_trigram, M):\n",
        "\n",
        "  trigrams = list(count_trigram.keys())\n",
        "  L=0\n",
        "  for trigram in trigrams:\n",
        "    if prob_trigram[trigram]!=0:\n",
        "      L += count_trigram[trigram]*np.log2(prob_trigram[trigram])\n",
        "\n",
        "  L = L/M\n",
        "  return pow(2,-1*L), L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAEOU_Yp4vXs",
        "outputId": "fff02d3b-ae7f-417b-f496-2d374184a1fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in range(5):\n",
        "\n",
        "  data = copy.deepcopy(train_dev_set)\n",
        "  random.shuffle(data)\n",
        "\n",
        "  division_index = floor(0.9*len(data))\n",
        "  train_set = data[:division_index]\n",
        "  dev_set = data[division_index:]\n",
        "  test_set_copy = copy.deepcopy(test_set)\n",
        "\n",
        "  trigram_list_train = []\n",
        "  bigram_list_train = []\n",
        "  unigram_list_train = []\n",
        "  \n",
        "  for sent in train_set:\n",
        "    unigram_list_train.extend(sent)\n",
        "    bigram_list_train.extend(list(nltk.bigrams(sent)))\n",
        "    trigram_list_train.extend(list(nltk.trigrams(sent)))\n",
        "\n",
        "  count_trigram_train = defaultdict(int,Counter(trigram_list_train))\n",
        "  count_bigram_train = defaultdict(int,Counter(bigram_list_train))\n",
        "  count_unigram_train = defaultdict(int,Counter(unigram_list_train))\n",
        "\n",
        "  trigram_list_dev = []\n",
        "  for sent in dev_set:\n",
        "    trigram_list_dev.extend(list(nltk.trigrams(sent)))\n",
        "  count_trigram_dev = defaultdict(int,Counter(trigram_list_dev))\n",
        "\n",
        "  trigram_list_test = []\n",
        "  for sent in test_set_copy:\n",
        "    trigram_list_test.extend(list(nltk.trigrams(sent)))\n",
        "  count_trigram_test = defaultdict(int,Counter(trigram_list_test))\n",
        "\n",
        "  no_of_words_dev = 0\n",
        "  for sent in dev_set:\n",
        "    no_of_words_dev += len(sent)\n",
        "\n",
        "  no_of_words_test = 0\n",
        "  for sent in test_set:\n",
        "    no_of_words_test += len(sent)\n",
        "\n",
        "  trigram_prob_laplace = get_trigram_probabilities(count_unigram_train, count_bigram_train, count_trigram_train,  len(vocab))\n",
        "  perplexity_dev, log_likelihood_dev = compute_perplexity(count_trigram_dev, trigram_prob_laplace, no_of_words_dev)\n",
        "  perplexity_test, log_likelihood_test = compute_perplexity(count_trigram_test, trigram_prob_laplace, no_of_words_test)\n",
        "\n",
        "  print('Iteration Number: ', i+1)\n",
        "  print('Log-likelihood for validation set =', log_likelihood_dev)\n",
        "  print('Perplexity for validation set =', perplexity_dev)\n",
        "  print('Log-likelihood for test set =', log_likelihood_test)\n",
        "  print('Perplexity for test set =', perplexity_test)\n",
        "  print('')\n",
        "\n",
        "  del data, train_set, dev_set, test_set_copy, trigram_list_train, bigram_list_train, unigram_list_train, count_trigram_train, count_bigram_train, count_unigram_train\n",
        "  del trigram_list_dev, count_trigram_dev, trigram_list_test, count_trigram_test, no_of_words_dev, no_of_words_test, trigram_prob_laplace, perplexity_dev, log_likelihood_dev, perplexity_test, log_likelihood_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration Number:  1\n",
            "Log-likelihood for validation set = -5.301196849844386\n",
            "Perplexity for validation set = 39.42931795259473\n",
            "Log-likelihood for test set = -5.241714391812473\n",
            "Perplexity for test set = 37.83670079104829\n",
            "\n",
            "Iteration Number:  2\n",
            "Log-likelihood for validation set = -5.500050513809676\n",
            "Perplexity for validation set = 45.25641855402469\n",
            "Log-likelihood for test set = -5.247508919888655\n",
            "Perplexity for test set = 37.9889760155538\n",
            "\n",
            "Iteration Number:  3\n",
            "Log-likelihood for validation set = -5.2845101565410095\n",
            "Perplexity for validation set = 38.975892565117846\n",
            "Log-likelihood for test set = -5.247515208829299\n",
            "Perplexity for test set = 37.9891416159955\n",
            "\n",
            "Iteration Number:  4\n",
            "Log-likelihood for validation set = -5.33363936343917\n",
            "Perplexity for validation set = 40.326026803713866\n",
            "Log-likelihood for test set = -5.249774147794518\n",
            "Perplexity for test set = 38.048670739337226\n",
            "\n",
            "Iteration Number:  5\n",
            "Log-likelihood for validation set = -5.301248277034215\n",
            "Perplexity for validation set = 39.43072349923013\n",
            "Log-likelihood for test set = -5.247980129037285\n",
            "Perplexity for test set = 38.00138589858815\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqgUuxNp7Vxf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}